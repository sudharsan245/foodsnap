# Food-Specific AI Image Detector - Domain Adaptation Plan

## Executive Summary

This document outlines the complete strategy for adapting the SMOGY AI-vs-Real image detection model into a **food-specific AI image detector** through targeted fine-tuning and domain adaptation.

**Core Principle**: Preserve the model's learned general AI-vs-Real texture knowledge while adapting it to food-specific scenarios, including real contamination cases.

---

## PHASE 1: Model Inspection & Freezing Strategy

### 1.1 SMOGY Architecture Overview

The SMOGY model is based on **Swin Transformer** architecture:

```
Input Image (224x224)
    â†“
Patch Embedding Layer (converts image to patches)
    â†“
Swin Transformer Blocks (4 stages)
    â”œâ”€â”€ Stage 1: Early features (edges, textures)
    â”œâ”€â”€ Stage 2: Mid-level features (patterns)
    â”œâ”€â”€ Stage 3: High-level features (objects)
    â””â”€â”€ Stage 4: Abstract features (semantic understanding)
    â†“
Classification Head (Linear layer â†’ 2 classes)
```

### 1.2 Layer Freezing Strategy

**Freeze (80-90% of parameters):**
- âœ… Patch embedding layers (preserve low-level feature extraction)
- âœ… Stage 1 & 2 transformer blocks (preserve texture/pattern detection)
- âœ… Stage 3 transformer blocks (preserve object understanding)

**Keep Trainable (10-20% of parameters):**
- ðŸ”“ Stage 4 transformer blocks (adapt to food-specific features)
- ðŸ”“ Classification head (learn food-specific decision boundaries)

**Rationale:**
- Early layers learn universal features (edges, textures, compression artifacts)
- These are critical for detecting AI generation artifacts
- Only final layers need adaptation for food domain specifics
- Prevents catastrophic forgetting of AI detection capabilities

### 1.3 Expected Trainable Parameters

```
Total Parameters: ~28M (Swin-Tiny)
Frozen Parameters: ~24M (85%)
Trainable Parameters: ~4M (15%)
```

---

## PHASE 2: Food-Specific Dataset Integration

### 2.1 Dataset Classes

**3-Class Classification:**

| Class | Label | Description | Examples |
|-------|-------|-------------|----------|
| 0 | Real Food (Clean) | Genuine food photos without contamination | Normal delivery photos |
| 1 | Real Food (Contaminated) | Genuine photos with visible contamination | Hair, insects, mold, plastic, metal, burnt food |
| 2 | AI-Generated Food | AI-created food images (clean or fake-contaminated) | Midjourney, DALL-E, Stable Diffusion outputs |

### 2.2 Dataset Requirements

**Real Contaminated Food (Class 1):**
- âœ… Visible hair in food
- âœ… Insects/bugs
- âœ… Mold/fungus
- âœ… Plastic pieces
- âœ… Metal fragments
- âœ… Burnt/charred food
- âœ… Foreign objects

**AI-Generated Food (Class 2):**
- âœ… Clean AI food images
- âœ… AI food with fake contamination
- âœ… Various AI generators (Midjourney, DALL-E, Stable Diffusion)
- âœ… Different styles (photorealistic, artistic)

### 2.3 Data Augmentation Pipeline

**Heavy Real-World Augmentations:**

```python
Augmentations:
â”œâ”€â”€ JPEG Compression (quality 60-95)
â”œâ”€â”€ Gaussian Blur (sigma 0.5-2.0)
â”œâ”€â”€ Random Crop/Zoom (0.8-1.0 scale)
â”œâ”€â”€ Low-light Simulation (brightness 0.5-1.5)
â”œâ”€â”€ Color Distortion (hue, saturation, contrast)
â”œâ”€â”€ Random Rotation (Â±15Â°)
â”œâ”€â”€ Perspective Transform
â””â”€â”€ Noise Injection (Gaussian, salt-pepper)
```

**Purpose**: Simulate real-world complaint image conditions (WhatsApp compression, poor lighting, phone camera quality)

### 2.4 Class Balancing

**Strategy:**
- Equal samples per class during training
- Oversample minority classes if needed
- Weighted loss function to handle imbalance

**Target Distribution:**
- Class 0 (Real Clean): 33%
- Class 1 (Real Contaminated): 33%
- Class 2 (AI-Generated): 34%

---

## PHASE 3: Fine-Tuning Procedure

### 3.1 Training Configuration

```yaml
Training Setup:
  optimizer: AdamW
  learning_rate: 1e-5  # Low LR for fine-tuning
  weight_decay: 0.01
  batch_size: 16
  epochs: 10-15
  loss: CrossEntropyLoss (with class weights)
  scheduler: CosineAnnealingLR
  warmup_steps: 500
```

### 3.2 Training Strategy

**Phase 3A: Initial Fine-Tuning (3-class)**
1. Train on balanced 3-class dataset
2. Monitor validation accuracy per class
3. Early stopping on validation loss plateau
4. Save best checkpoint

**Phase 3B: Optional Binary Conversion**
- Merge Class 0 & 1 â†’ REAL
- Class 2 â†’ AI-GENERATED
- Retrain classification head only (2-3 epochs)

### 3.3 Evaluation Metrics

**Primary Metrics:**
- Precision (minimize false positives on real food)
- Recall (catch AI-generated images)
- F1-Score per class
- Confusion Matrix

**Critical Constraint:**
- **False Positive Rate on Real Food < 5%**
- Better to miss AI images than wrongly reject real complaints

### 3.4 Overfitting Prevention

- âœ… Heavy augmentation
- âœ… Dropout in classification head
- âœ… Early stopping
- âœ… Validation on held-out set
- âœ… Test on completely different data sources

---

## PHASE 4: Food-Specific Negative Knowledge

### 4.1 Hard Negative Examples

**Include in training:**

| Hard Negative Type | Why It's Hard | Solution |
|-------------------|---------------|----------|
| Real food with heavy Instagram filters | May look "too perfect" | Include in Class 0/1 with augmentation |
| Screenshots of food photos | Compression artifacts similar to AI | Explicit screenshot examples |
| Printed photos re-captured | Texture changes confuse model | Include re-photographed prints |
| AI food post-processed to look real | Noise/blur added to AI images | Include adversarial AI examples |
| Extreme close-ups | Limited context | Crop augmentation on real images |

### 4.2 Adversarial Training

**Strategy:**
1. Generate AI food images
2. Post-process with noise, blur, compression
3. Include as Class 2 (AI) in training
4. Forces model to detect subtle AI artifacts

### 4.3 Expected Robustness Gains

- âœ… Reduced false positives on filtered real photos
- âœ… Better handling of low-quality images
- âœ… Resistance to simple post-processing tricks
- âœ… Improved generalization to new AI generators

---

## PHASE 5: Real-World Validation

### 5.1 Test Conditions

**Simulate Real Complaint Scenarios:**

| Test Scenario | Image Source | Expected Challenge |
|--------------|--------------|-------------------|
| WhatsApp screenshots | Compressed, re-encoded | Multiple compression layers |
| Zomato/Swiggy uploads | Mobile app compression | Platform-specific artifacts |
| Low-resolution images | <500px | Limited detail |
| Cropped food regions | Only food, no context | Missing environmental cues |
| Night/low-light photos | Poor lighting | High noise, low contrast |

### 5.2 Evaluation Metrics

**Critical Metrics:**

```python
Metrics:
â”œâ”€â”€ Precision (Real Food): > 95%
â”œâ”€â”€ Recall (AI Food): > 80%
â”œâ”€â”€ False Positive Rate: < 5%
â”œâ”€â”€ Confidence Score Distribution
â”‚   â”œâ”€â”€ Real Food: Mean confidence > 0.7
â”‚   â””â”€â”€ AI Food: Mean confidence > 0.8
â””â”€â”€ Threshold Analysis
    â”œâ”€â”€ P(AI) < 0.60 â†’ Accept
    â”œâ”€â”€ 0.60 â‰¤ P(AI) < 0.80 â†’ Manual Review
    â””â”€â”€ P(AI) â‰¥ 0.80 â†’ Likely AI
```

### 5.3 Failure Case Analysis

**Document and analyze:**
- False positives (real food flagged as AI)
- False negatives (AI food accepted as real)
- Edge cases requiring manual review
- Systematic biases (e.g., certain cuisines)

### 5.4 Threshold Calibration

**Adjust thresholds based on business requirements:**

| Threshold Set | Use Case | FPR | Recall |
|--------------|----------|-----|--------|
| Conservative | Customer-first approach | <2% | ~70% |
| Balanced | Default setting | ~5% | ~85% |
| Aggressive | High fraud environment | ~10% | ~95% |

---

## FINAL CONSTRAINTS & INTEGRATION

### Constraints

âœ… **No EXIF/Metadata**: Detection is purely pixel-based  
âœ… **No Redesign**: Integrates with existing Flask/frontend  
âœ… **Seamless Integration**: Drop-in replacement for current model  
âœ… **Backward Compatible**: Same API interface  

### Integration Points

```python
# detector.py - No changes to API
detector = FoodImageDetector()  # Now loads fine-tuned model
result = detector.predict("image.jpg")  # Same interface

# Only change: MODEL_ID in config.py
MODEL_ID = "path/to/fine-tuned-food-model"
```

### Deployment Strategy

1. Train and validate fine-tuned model
2. Save model to local path or Hugging Face Hub
3. Update `MODEL_ID` in `config.py`
4. Test with existing Flask app
5. Deploy with same infrastructure

---

## FINAL EXPLANATION

**"The model is a food-domainâ€“fine-tuned Swin Transformer that detects AI-generated food images by learning the absence of physical cooking chaos and camera sensor randomness."**

### What This Means:

**Real Food Photos Contain:**
- âœ… Camera sensor noise patterns
- âœ… Natural lighting inconsistencies
- âœ… Physical texture randomness (steam, grease, crumbs)
- âœ… Environmental context (plates, tables, hands)
- âœ… Compression artifacts from real camera â†’ upload pipeline

**AI-Generated Food Images Lack:**
- âŒ True sensor noise (synthetic noise is different)
- âŒ Physical cooking chaos (too perfect, unrealistic lighting)
- âŒ Authentic texture randomness (patterns are learned, not physical)
- âŒ Real-world imperfections (scratches on plates, fingerprints)
- âŒ Natural compression artifacts (AI â†’ save â†’ upload has different signature)

**The Fine-Tuned Model Learns:**
- ðŸŽ¯ Food-specific AI generation artifacts
- ðŸŽ¯ Difference between real contamination and AI-faked contamination
- ðŸŽ¯ Robustness to real-world image degradation
- ðŸŽ¯ Conservative decision boundaries (favor customers)

---

## Next Steps

1. **Implement model inspection script** (`inspect_model.py`)
2. **Create dataset loader** (`dataset.py`)
3. **Implement fine-tuning script** (`finetune.py`)
4. **Build evaluation pipeline** (`evaluate.py`)
5. **Test integration** with existing Flask app

---

**Document Version**: 1.0  
**Last Updated**: 2026-01-30  
**Status**: Ready for Implementation
